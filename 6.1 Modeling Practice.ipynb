{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27f18255-a7af-4c65-bd8e-f6f7003223ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import md5, concat_ws, col, expr, to_date, to_timestamp\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType,\n",
    "    StringType, TimestampType, DoubleType, LongType, DoubleType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b967aa5e-7a2b-4ddf-a4d6-63eb57395e14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "build lake_sales"
    }
   },
   "outputs": [],
   "source": [
    "customer_schema = ('''\n",
    "    row_key STRING PRIMARY KEY NOT NULL COMMENT \"Unique key for each record\",\n",
    "    customer_id INT NOT NULL,\n",
    "    customer_name STRING,\n",
    "    country STRING,\n",
    "    md5_key STRING,\n",
    "    __START_AT STRING,\n",
    "    __END_AT STRING\n",
    "  ''')\n",
    "\n",
    "@dlt.view\n",
    "def raw_customer():\n",
    "  df = spark.readStream.table(\"raw_schema.raw_customer\")\n",
    "  return df.withColumn(\"row_key\", md5(concat_ws(\"|\", col(\"customer_id\").cast(\"string\")))) \\\n",
    "          .withColumn('customer_id', col(\"customer_id\").cast(\"int\")) \\\n",
    "          .withColumn('customer_name', col(\"name\").cast(\"string\")) \\\n",
    "          .withColumn(\"md5_key\", md5(concat_ws(\"|\", col(\"customer_name\"), col(\"country\")))) \\\n",
    "          .select(\"row_key\", \"customer_id\", \"customer_name\", \"country\", \"md5_key\",\"db_timestamp\", \"marker\")\n",
    "\n",
    "dlt.create_streaming_table(\n",
    "    name=\"lake_schema.gmsgq_customers\",\n",
    "    comment=\"Customer table with SCD2\",\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    schema=customer_schema,\n",
    ")\n",
    "\n",
    "dlt.create_auto_cdc_flow(\n",
    "  target = \"lake_schema.gmsgq_customers\",\n",
    "  source = \"raw_customer\",\n",
    "  keys = [\"row_key\"],\n",
    "  sequence_by = col(\"db_timestamp\"),\n",
    "  apply_as_deletes = expr(\"marker = 'D'\"),\n",
    "  except_column_list = [\"marker\", \"db_timestamp\"],\n",
    "  stored_as_scd_type = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d160b23-fb50-4694-bede-dc41d7ed798b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "build lake_sales"
    }
   },
   "outputs": [],
   "source": [
    "# Define DDL schema for the sales table\n",
    "sales_schema = ('''\n",
    "  row_key STRING PRIMARY KEY NOT NULL COMMENT \"Unique key per row for SCD1 overwrite\",\n",
    "  customer_id INTEGER NOT NULL,\n",
    "  product_id INTEGER NOT NULL,\n",
    "  sale_date DATE,\n",
    "  amount DOUBLE,\n",
    "  md5_key STRING\n",
    "''')\n",
    "\n",
    "# Create a DLT view from the raw source with required transformations\n",
    "@dlt.view\n",
    "def raw_sales():\n",
    "  df = spark.readStream.table(\"raw_schema.raw_sales\")\n",
    "  return df \\\n",
    "    .withColumn(\"customer_id\", col(\"customer_id\").cast(\"int\")) \\\n",
    "    .withColumn(\"product_id\", col(\"product_id\").cast(\"int\")) \\\n",
    "    .withColumn(\"sale_date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "    .withColumn(\"db_timestamp\", to_timestamp(col(\"db_timestamp\"))) \\\n",
    "    .withColumn(\"row_key\", md5(concat_ws(\"|\", col(\"customer_id\").cast(\"string\"), col(\"product_id\").cast(\"string\")))) \\\n",
    "    .withColumn(\"md5_key\", md5(concat_ws(\"|\", col(\"sale_date\").cast(\"string\"), col(\"amount\").cast(\"string\")))) \\\n",
    "    .select(\"row_key\", \"customer_id\", \"product_id\", \"sale_date\", \"amount\", \"md5_key\", \"db_timestamp\", \"marker\")\n",
    "\n",
    "# Register a streaming DLT table with schema\n",
    "dlt.create_streaming_table(\n",
    "    name=\"lake_schema.gmsgq_sales\",\n",
    "    comment=\"Sales table with SCD1 (latest row per key)\",\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    schema=sales_schema,\n",
    ")\n",
    "\n",
    "# Define the SCD1 merge logic using auto_cdc_flow\n",
    "dlt.create_auto_cdc_flow(\n",
    "  target = \"lake_schema.gmsgq_sales\",\n",
    "  source = \"raw_sales\",\n",
    "  keys = [\"row_key\"],\n",
    "  sequence_by = col(\"db_timestamp\"),\n",
    "  apply_as_deletes = expr(\"marker = 'D'\"),\n",
    "  except_column_list = [\"marker\",\"db_timestamp\"],\n",
    "  stored_as_scd_type = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37c53a78-c0b4-4d0f-b903-fec196f2d81a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "build lake_product"
    }
   },
   "outputs": [],
   "source": [
    "product_schema = ('''\n",
    "  row_key STRING PRIMARY KEY NOT NULL COMMENT \"Deterministic row key for SCD1 deduplication\",\n",
    "  product_id INT NOT NULL,\n",
    "  product_name STRING,\n",
    "  product_category STRING,\n",
    "  md5_key STRING\n",
    "''')\n",
    "\n",
    "@dlt.view\n",
    "def raw_product():\n",
    "  df = spark.readStream.table(\"raw_schema.raw_product\")\n",
    "  return df \\\n",
    "    .withColumn(\"product_id\", col(\"product_id\").cast(\"int\")) \\\n",
    "    .withColumn(\"product_category\", col(\"category\").cast(\"string\")) \\\n",
    "    .withColumn(\"db_timestamp\", to_timestamp(col(\"db_timestamp\"))) \\\n",
    "    .withColumn(\"row_key\", md5(col(\"product_id\").cast(\"string\"))) \\\n",
    "    .withColumn(\"md5_key\", md5(concat_ws(\"|\", col(\"product_name\"), col(\"category\")))) \\\n",
    "    .select(\"row_key\", \"product_id\", \"product_name\", \"product_category\", \"md5_key\", \"db_timestamp\", \"marker\")\n",
    "\n",
    "dlt.create_streaming_table(\n",
    "    name=\"lake_schema.gmsgq_products\",\n",
    "    comment=\"Product table with SCD1 overwrite (latest version per product_id)\",\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    schema=product_schema,\n",
    ")\n",
    "\n",
    "dlt.create_auto_cdc_flow(\n",
    "  target = \"lake_schema.gmsgq_products\",\n",
    "  source = \"raw_product\",\n",
    "  keys = [\"row_key\"],\n",
    "  sequence_by = col(\"db_timestamp\"),\n",
    "  apply_as_deletes = expr(\"marker = 'D'\"),\n",
    "  except_column_list = [\"marker\",\"db_timestamp\"],\n",
    "  stored_as_scd_type = 1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6.1 Modeling Practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
