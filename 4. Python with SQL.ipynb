{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e5f66d-cd16-4278-a5ba-2d2e3fa9bcf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW sales_demo AS\n",
    "SELECT * FROM VALUES\n",
    "  ('2025-07-01', 'Apples', 10, 2.5, 'North'),\n",
    "  ('2025-07-01', 'Oranges', 5, 3.0, 'North'),\n",
    "  ('2025-07-02', 'Apples', 8, 2.5, 'South'),\n",
    "  ('2025-07-02', 'Bananas', 15, 1.2, NULL),\n",
    "  ('2025-07-03', 'Oranges', 7, 3.0, 'East')\n",
    "AS sales(date, product, quantity, price_per_unit, region);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdc8f495-e151-4eb6-9f66-e7a7776a912d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸš€ Using Python to Parameterize Complex Spark SQL Queries\n",
    "\n",
    "In Databricks, combining Python with Spark SQL lets you build dynamic, reusable queries with parameters â€” perfect for complex analytics or pipeline workflows.\n",
    "\n",
    "### Why parameterize?\n",
    "\n",
    "- Reuse SQL code without hardcoding values\n",
    "- Easily pass variables from Python logic (dates, IDs, filters)\n",
    "- Build complex queries programmatically and debug more easily\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Parameterizing a Complex Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db46db2b-27e7-4218-844b-7f2be443e6d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL query"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Define parameters in Python\n",
    "start_date = '2025-07-01'\n",
    "end_date = '2025-07-03'\n",
    "region_filter = 'North'\n",
    "\n",
    "# Use triple quotes for multiline SQL, f-string for injecting variables safely\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  date,\n",
    "  product,\n",
    "  quantity,\n",
    "  price_per_unit,\n",
    "  region,\n",
    "  CASE \n",
    "    WHEN quantity > 10 THEN 'High'\n",
    "    WHEN quantity > 5 THEN 'Medium'\n",
    "    ELSE 'Low'\n",
    "  END AS demand_level\n",
    "FROM sales_demo\n",
    "WHERE date BETWEEN '{start_date}' AND '{end_date}'\n",
    "  AND region = '{region_filter}'\n",
    "\"\"\"\n",
    "\n",
    "# Run the query via spark.sql()\n",
    "df = spark.sql(query)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf34e60c-c108-4043-a4f9-2f3c1b246c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’¡ Important !!\n",
    "\n",
    "When building Delta Live Tables (DLT) pipelines or any scalable data workflows in Databricks, the ability to parameterize Spark SQL queries is **crucial**. We can, and we do, setup parameters in our JSON configuration file for DLT pipeline build. These parameters are used in table and pipeline creation.\n",
    "\n",
    "### Key reasons:\n",
    "\n",
    "- **Centralized logic management:**  \n",
    "  Instead of duplicating SQL queries for each variation (e.g., different Sites or regions), you write one parameterized query. Changing it in one place automatically propagates to all use cases.\n",
    "\n",
    "- **Dynamic pipeline building:**  \n",
    "  Each DLT table is created using Spark SQL logic, but behind the scenes, this logic is often enriched with audit columns, business keys, or hashed surrogate keys (like MD5 of concatenated keys). Being able to inject parameters programmatically lets you apply these transformations consistently and dynamically.\n",
    "\n",
    "- **Standardization and reusability:**  \n",
    "  Standardization functions (e.g., for data cleansing or harmonization) can be applied dynamically within parameterized queries or UDFs. This avoids manual, repetitive updates and helps maintain data quality.\n",
    "\n",
    "- **Simplified maintenance:**  \n",
    "  When business requirements change (e.g., adding a new region or modifying audit logic), you only update the parameterized SQL once â€” no need to hunt down multiple hardcoded queries."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4. Python with SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
