{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e5f66d-cd16-4278-a5ba-2d2e3fa9bcf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW sales_demo AS\n",
    "SELECT * FROM VALUES\n",
    "  ('2025-07-01', 'Apples', 10, 2.5, 'North'),\n",
    "  ('2025-07-01', 'Oranges', 5, 3.0, 'North'),\n",
    "  ('2025-07-02', 'Apples', 8, 2.5, 'South'),\n",
    "  ('2025-07-02', 'Bananas', 15, 1.2, NULL),\n",
    "  ('2025-07-03', 'Oranges', 7, 3.0, 'East')\n",
    "AS sales(date, product, quantity, price_per_unit, region);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc8f495-e151-4eb6-9f66-e7a7776a912d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üöÄ Using Python to Parameterize Complex Spark SQL Queries\n",
    "\n",
    "In Databricks, combining Python with Spark SQL lets you build dynamic, reusable queries with parameters ‚Äî perfect for complex analytics or pipeline workflows.\n",
    "\n",
    "### Why parameterize?\n",
    "\n",
    "- Reuse SQL code without hardcoding values\n",
    "- Easily pass variables from Python logic (dates, IDs, filters)\n",
    "- Build complex queries programmatically and debug more easily\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Parameterizing a Complex Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db46db2b-27e7-4218-844b-7f2be443e6d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL query"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Define parameters in Python\n",
    "start_date = '2025-07-01'\n",
    "end_date = '2025-07-03'\n",
    "region_filter = 'North'\n",
    "\n",
    "# Use triple quotes for multiline SQL, f-string for injecting variables safely\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  date,\n",
    "  product,\n",
    "  quantity,\n",
    "  price_per_unit,\n",
    "  region,\n",
    "  CASE \n",
    "    WHEN quantity > 10 THEN 'High'\n",
    "    WHEN quantity > 5 THEN 'Medium'\n",
    "    ELSE 'Low'\n",
    "  END AS demand_level\n",
    "FROM sales_demo\n",
    "WHERE date BETWEEN '{start_date}' AND '{end_date}'\n",
    "  AND region = '{region_filter}'\n",
    "\"\"\"\n",
    "\n",
    "# Run the query via spark.sql()\n",
    "df = spark.sql(query)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf34e60c-c108-4043-a4f9-2f3c1b246c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Important !!\n",
    "\n",
    "When building Delta Live Tables (DLT) pipelines or any scalable data workflows in Databricks, the ability to parameterize Spark SQL queries is **crucial**. We can, and we do, setup parameters in our JSON configuration file for DLT pipeline build. These parameters are used in table and pipeline creation.\n",
    "\n",
    "### Key reasons:\n",
    "\n",
    "- **Centralized logic management:**  \n",
    "  Instead of duplicating SQL queries for each variation (e.g., different Sites or regions), you write one parameterized query. Changing it in one place automatically propagates to all use cases.\n",
    "\n",
    "- **Dynamic pipeline building:**  \n",
    "  Each DLT table is created using Spark SQL logic, but behind the scenes, this logic is often enriched with audit columns, business keys, or hashed surrogate keys (like MD5 of concatenated keys). Being able to inject parameters programmatically lets you apply these transformations consistently and dynamically.\n",
    "\n",
    "- **Standardization and reusability:**  \n",
    "  Standardization functions (e.g., for data cleansing or harmonization) can be applied dynamically within parameterized queries or UDFs. This avoids manual, repetitive updates and helps maintain data quality.\n",
    "\n",
    "- **Simplified maintenance:**  \n",
    "  When business requirements change (e.g., adding a new region or modifying audit logic), you only update the parameterized SQL once ‚Äî no need to hunt down multiple hardcoded queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15903be4-8bf1-48e5-b73e-6649ac518f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üì¶ Collecting Scalar Values from DataFrames Safely in PySpark\n",
    "In this section, we demonstrate how to **safely extract a single value from a PySpark DataFrame** and use it in another operation ‚Äî without using `.collect()`, which can overwhelm the driver with large data.\n",
    "\n",
    "## üßÆ Goal:\n",
    "Find the product with the **highest quantity sold** from a DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why Avoid `.collect()`?\n",
    "- `df.collect()` loads **all rows** into the driver ‚Äî not efficient or safe for large datasets.\n",
    "- Instead, use `.select()` and `.first()` to bring just **one row** (scalar value) to the driver.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Step-by-Step Logic:\n",
    "\n",
    "1. **Select the maximum value** using Spark SQL function `max()`.\n",
    "2. **Show and inspect** the intermediate result (optional debugging).\n",
    "3. **Extract the max value** into a Python variable using `.first()`.\n",
    "4. **Access the scalar** either by column name or index.\n",
    "5. **Use the scalar in a filter**, then extract the product with the highest quantity.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Use Case in Real Projects:\n",
    "This is a key pattern when building **DLT pipelines** or **parameterized logic**:\n",
    "- You often want to calculate a metric in Spark (e.g. latest date, highest score, etc.)\n",
    "- Then use that scalar in further logic, conditions, or joins.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ°Ô∏è Summary Table\n",
    "\n",
    "| ‚úÖ Best Practice                  | ‚ùå Risky Practice              |\n",
    "|----------------------------------|-------------------------------|\n",
    "| `df.select(...).first()`         | `df.collect()`                |\n",
    "| Pulls one row (scalar value)     | Pulls **all** rows to driver  |\n",
    "| Safe for production workflows    | Risk of memory overload       |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a7b74a-301d-4e36-a8e6-aae0bad41d6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "# üîç **Step 1: Calculate the max quantity as a new DataFrame**\n",
    "max_quantity_dataframe = df.select(max(\"quantity\"))\n",
    "\n",
    "# üì∫ **Step 2: Debug ‚Äî show and display the intermediate max result**\n",
    "print(\"üîé DataFrame containing max quantity:\")\n",
    "print(max_quantity_dataframe)  # <-- just the logical plan\n",
    "max_quantity_dataframe.show()  # <-- tabular display\n",
    "display(max_quantity_dataframe)  # <-- for notebook UI\n",
    "\n",
    "# üß± **Step 3: Extract the max value as a Row object**\n",
    "max_quantity = df.select(max(\"quantity\")).first()\n",
    "print(\"üì¶ Row object from which we'll extract the scalar value:\")\n",
    "print(max_quantity)\n",
    "\n",
    "# üéØ **Step 4: Extract scalar value by name and index**\n",
    "max_quantity_by_name = max_quantity[\"max(quantity)\"]\n",
    "max_quantity_by_index = max_quantity[0]\n",
    "print(f\"üî¢ Extracted max quantity (by column name): {max_quantity_by_name}\")\n",
    "print(f\"üî¢ Extracted max quantity (by index): {max_quantity_by_index}\")\n",
    "\n",
    "# üèÜ **Step 5: Use the extracted value to find the product with highest quantity**\n",
    "highest_quantity_product = (\n",
    "    df.filter(df[\"quantity\"] == max_quantity_by_index)\n",
    "    .select(\"product\")\n",
    "    .first()[0]\n",
    ")\n",
    "print(f\"üèÜ Product with the highest quantity sold: **{highest_quantity_product}**\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4. Python with SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
