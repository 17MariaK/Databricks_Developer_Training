{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9264d5e5-6223-4cec-9bfe-f38c10a30cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## building prerequisites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a5eff3c-0e98-441e-a629-a06e030931c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "employee_hierarchy"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW employee_hierarchy AS\n",
    "SELECT * FROM VALUES\n",
    "  (1, 'CEO', NULL),\n",
    "  (2, 'CTO', 1),\n",
    "  (3, 'CFO', 1),\n",
    "  (4, 'Engineering Manager', 2),\n",
    "  (5, 'Finance Manager', 3),\n",
    "  (6, 'Software Engineer', 4),\n",
    "  (7, 'Accountant', 5)\n",
    "AS employees(employee_id, employee_name, manager_id);\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW orders_with_items AS\n",
    "SELECT * FROM VALUES\n",
    "  (1, '2025-07-01', ARRAY('apple', 'banana', 'orange')),\n",
    "  (2, '2025-07-02', ARRAY('grape', 'kiwi')),\n",
    "  (3, '2025-07-03', ARRAY())\n",
    "AS orders(order_id, order_date, items);\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW products_with_prices AS\n",
    "SELECT * FROM VALUES\n",
    "  (1, MAP('apple', 1.5, 'banana', 0.75)),\n",
    "  (2, MAP('grape', 2.0, 'kiwi', 1.8)),\n",
    "  (3, MAP())\n",
    "AS products(product_id, price_map);\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW employees_with_address AS\n",
    "SELECT * FROM VALUES\n",
    "  (1, STRUCT('John Doe' AS name, '123 Main St' AS street, 'NY' AS state)),\n",
    "  (2, STRUCT('Jane Smith' AS name, '456 Park Ave' AS street, 'CA' AS state)),\n",
    "  (3, NULL)\n",
    "AS employees(employee_id, address);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3050ad20-c859-4759-843b-90e75d81e68a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚è≥ Delta Lake Features: Time Travel and Versioning\n",
    "\n",
    "Delta Lake brings powerful features on top of Apache Spark and Parquet to enable **reliable**, **ACID-compliant** data lakes.\n",
    "\n",
    "One of its key capabilities is **time travel**, allowing you to query, compare, and restore data from previous versions of a Delta table effortlessly.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è What is Time Travel?\n",
    "\n",
    "- Delta Lake stores all changes and versions of a table as data files and transaction logs.\n",
    "- You can query the table ‚Äúas of‚Äù a previous **version number** or **timestamp**.\n",
    "- This enables:\n",
    "  - Auditing data changes\n",
    "  - Debugging by comparing versions\n",
    "  - Undoing accidental deletes or corruptions\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Comparing Two Versions to Find New Rows\n",
    "\n",
    "You can compare two versions of a table to identify:\n",
    "\n",
    "- Rows added\n",
    "- Rows deleted\n",
    "- Rows updated\n",
    "\n",
    "For example, to find rows that were added in version N compared to version N-1.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Reverting to a Previous Version\n",
    "\n",
    "If you find issues in the latest data, you can restore the table to a previous version by **writing over** with that version‚Äôs data or using Delta‚Äôs `RESTORE` command (Databricks Premium).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Benefits of Time Travel\n",
    "\n",
    "- Simplifies data recovery and compliance.\n",
    "- Enables auditability and reproducibility.\n",
    "- Supports complex data pipelines with rollback and snapshot functionality.\n",
    "\n",
    "---\n",
    "\n",
    "Next, let's see how to use these features with SQL examples!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebacee90-4742-4164-9043-c2c5d59c327d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "initial load V0"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE delta_demo (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  value INT\n",
    ") USING DELTA;                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4be302d3-55b8-459b-9696-7a1e3e871384",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "first insert of values V1"
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO delta_demo VALUES\n",
    "  (1, 'A', 100),\n",
    "  (2, 'B', 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abc9c17-6a1b-482f-8dd9-b3c6839b768d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DESCRIBE history V0 and V1 visible"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY delta_demo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3998776-ff42-4e3f-9664-079c521c49cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "another insert V2"
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO delta_demo VALUES\n",
    "  (3, 'C', 300),\n",
    "  (4, 'D', 400);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51dd9d6c-7822-41cf-9f82-45373b0ba309",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "timetravel to V1"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM delta_demo VERSION AS OF 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f4ed92-42e9-47e6-9ee9-20f61f81953d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "select current version"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM delta_demo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f6a063-1e32-4f13-95d9-91223fb91187",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "compare versions"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM delta_demo VERSION AS OF 1\n",
    "EXCEPT\n",
    "SELECT * FROM delta_demo VERSION AS OF 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba03121-61c0-4bea-97ab-3e4605c207da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "clean session"
    }
   },
   "outputs": [],
   "source": [
    "drop table if exists delta_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f73f8477-45d4-4a3a-9bb1-45daf862c816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üßπ Delta Lake VACUUM: Cleaning Up Old Data Files\n",
    "\n",
    "When you perform many updates, deletes, or time travel operations, Delta Lake keeps older data files and transaction logs to support querying previous versions. Over time, these can consume storage space.\n",
    "\n",
    "**VACUUM** helps you clean up these obsolete files safely.\n",
    "\n",
    "---\n",
    "\n",
    "### How VACUUM Works\n",
    "\n",
    "- Deletes files no longer needed by any Delta table version.\n",
    "- Retains data files required for time travel based on a retention period.\n",
    "- Default retention is 7 days to avoid accidental data loss.\n",
    "- You can specify a shorter retention period, but use caution!\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Example of VACUUM Usage\n",
    "\n",
    "\n",
    "-- Clean up files no longer needed by any active Delta table version older than 7 days\n",
    "\n",
    "`VACUUM delta_demo;`\n",
    "\n",
    "-- Or specify retention period in hours (e.g., 168 hours = 7 days)\n",
    "\n",
    "`VACUUM delta_demo RETAIN 168 HOURS;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dab4f20-0245-452c-986f-40d111541810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üîÑ Recursive CTEs in Spark SQL\n",
    "\n",
    "---\n",
    "\n",
    "### What is a Recursive CTE?\n",
    "\n",
    "- Recursive Common Table Expressions (CTEs) let you **iterate over hierarchical or graph-structured data**.\n",
    "- They repeatedly apply a query to generate rows based on the previous result until no new rows are returned.\n",
    "- Useful for things like **organizational charts, bill of materials, or folder hierarchies**.\n",
    "\n",
    "---\n",
    "\n",
    "### How it Works\n",
    "\n",
    "1. **Anchor member**: Defines the base rows (starting point).\n",
    "2. **Recursive member**: Joins the recursive CTE to itself to fetch \"next level\" rows.\n",
    "3. Spark SQL executes the recursive query repeatedly, accumulating results.\n",
    "4. The recursion ends when no more rows are generated.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Employee Hierarchy\n",
    "\n",
    "Using the `employee_hierarchy` view, find **all employees under the CEO (employee_id = 1)**, including their level in the hierarchy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73281fde-1894-46c6-983e-27928b824afc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "recursive CTE"
    }
   },
   "outputs": [],
   "source": [
    "WITH RECURSIVE emp_cte AS (\n",
    "  -- Anchor member: start with the CEO\n",
    "  SELECT employee_id, employee_name, manager_id, 0 AS level\n",
    "  FROM employee_hierarchy\n",
    "  WHERE employee_id = 1\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  -- Recursive member: get employees reporting to previous level\n",
    "  SELECT e.employee_id, e.employee_name, e.manager_id, cte.level + 1\n",
    "  FROM employee_hierarchy e\n",
    "  JOIN emp_cte cte ON e.manager_id = cte.employee_id\n",
    ")\n",
    "\n",
    "SELECT * FROM emp_cte ORDER BY level, employee_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8d1804c-7c64-4e3a-aee1-85464098ba7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ‚ö†Ô∏è Important Notice on Recursive CTEs in Databricks Free Edition\n",
    "\n",
    "Databricks Free Edition **does not allow** running recursive CTEs, which means you cannot use the elegant recursive query syntax to traverse hierarchical data.\n",
    "\n",
    "---\n",
    "\n",
    "### Alternative Approach: Emulating Recursive CTEs with Multiple Self-Joins\n",
    "\n",
    "The code below produces the **same output** as a recursive CTE for up to 4 hierarchy levels. However, it is:\n",
    "\n",
    "- **Less elegant**\n",
    "- **Not dynamic** (requires manually adding joins for each level)\n",
    "- **Harder to maintain** for deep or unknown hierarchy depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58f2bfd-a7ae-463b-a3d7-445d7b6f21c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "simulate recursive CTE"
    }
   },
   "outputs": [],
   "source": [
    "-- Level 0: CEO\n",
    "SELECT\n",
    "  e0.employee_id AS employee_id,\n",
    "  e0.employee_name AS employee_name,\n",
    "  e0.manager_id AS manager_id,\n",
    "  0 AS level\n",
    "FROM employee_hierarchy e0\n",
    "WHERE e0.employee_id = 1\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Level 1: Direct reports to CEO\n",
    "SELECT\n",
    "  e1.employee_id,\n",
    "  e1.employee_name,\n",
    "  e1.manager_id,\n",
    "  1 AS level\n",
    "FROM employee_hierarchy e1\n",
    "WHERE e1.manager_id = 1\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Level 2: Reports to level 1 employees\n",
    "SELECT\n",
    "  e2.employee_id,\n",
    "  e2.employee_name,\n",
    "  e2.manager_id,\n",
    "  2 AS level\n",
    "FROM employee_hierarchy e2\n",
    "JOIN employee_hierarchy e1 ON e2.manager_id = e1.employee_id\n",
    "WHERE e1.manager_id = 1\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Level 3: Reports to level 2 employees\n",
    "SELECT\n",
    "  e3.employee_id,\n",
    "  e3.employee_name,\n",
    "  e3.manager_id,\n",
    "  3 AS level\n",
    "FROM employee_hierarchy e3\n",
    "JOIN employee_hierarchy e2 ON e3.manager_id = e2.employee_id\n",
    "JOIN employee_hierarchy e1 ON e2.manager_id = e1.employee_id\n",
    "WHERE e1.manager_id = 1\n",
    "\n",
    "ORDER BY level, employee_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d41d0932-1714-457f-a437-777615ed3a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìú SQL UDFs with Unity Catalog\n",
    "\n",
    "## What is a SQL UDF?\n",
    "\n",
    "A **User Defined Function (UDF)** lets you extend SQL with your own reusable functions.  \n",
    "You can encapsulate complex logic inside a function, then call it in your queries just like built-in SQL functions.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of SQL UDFs\n",
    "\n",
    "- Simplify and reuse complex expressions or business logic\n",
    "- Improve query readability\n",
    "- Centralize logic to avoid duplication\n",
    "- Managed and secured with **Unity Catalog** for fine-grained access control and auditing\n",
    "\n",
    "---\n",
    "\n",
    "## Unity Catalog and SQL UDFs\n",
    "\n",
    "Unity Catalog allows you to:\n",
    "\n",
    "- Register UDFs with a fully qualified name (catalog.schema.function)\n",
    "- Control access to UDFs via permissions\n",
    "- Track usage for auditing and governance\n",
    "- Easily share functions across workspaces and users\n",
    "\n",
    "---\n",
    "\n",
    "## Syntax to Create SQL UDFs in Unity Catalog\n",
    "\n",
    "```sql\n",
    "CREATE FUNCTION catalog_name.schema_name.function_name(argument_name data_type, ...) \n",
    "RETURNS return_data_type\n",
    "LANGUAGE SQL\n",
    "AS \n",
    "'SQL expression or query using arguments';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187c8cc6-96a0-4c0c-a386-209c6c47269a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UDF"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION full_employee_info(emp_id INT, emp_name STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "RETURN CONCAT(emp_id, \"_\", emp_name);\n",
    "\n",
    "CREATE OR REPLACE FUNCTION is_top_level(emp_id INT)\n",
    "RETURNS BOOLEAN\n",
    "LANGUAGE SQL\n",
    "RETURN\n",
    "CASE WHEN emp_id IS NULL\n",
    " THEN TRUE\n",
    " ELSE FALSE\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f92367-bf18-48ab-a17d-c4444f308565",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "use UDF"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  employee_id,\n",
    "  employee_name,\n",
    "  full_employee_info(employee_id,employee_name) AS full_employee_info,\n",
    "  is_top_level(manager_id) AS is_ceo\n",
    "FROM employee_hierarchy;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e9ee69-e480-454e-b4e2-59140d97092e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üßπ Before Cleaning Up UDFs, Check Unity Catalog! \n",
    "You should beable to find functions besides tables... Nice, isnt it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2605bf5c-08b1-43dc-b252-72154479c645",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "clean UDFs"
    }
   },
   "outputs": [],
   "source": [
    "drop function if exists full_employee_info;\n",
    "drop function if exists is_top_level;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4d70e67-7c94-40f4-a1f9-c06e680e284d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Important !!\n",
    "\n",
    "**UDFs can also load complex data types like JSON objects**  \n",
    "https://docs.databricks.com/aws/en/udf/unity-catalog#extend-udfs-using-custom-dependencies\n",
    "\n",
    "(for example, map or array structures from mapping tables) and act as fully-fledged dictionaries for data standardization.\n",
    "\n",
    "This is very powerful because:  \n",
    "- You can implement reusable **standardization** or **harmonization** logic inside UDFs.  \n",
    "- Under the hood, such engines often leverage **Unity Catalog UDFs** to provide consistent, centralized, and maintainable data transformations across your pipelines.  \n",
    "\n",
    "In our project, we have a standardization/harmonization engine built this way ‚Äî using UDFs to apply consistent business rules and mappings seamlessly.\n",
    "\n",
    "This approach simplifies your ETL code and improves data quality by enforcing consistent rules via reusable functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d4a9739-268b-4cef-bf97-860420e139f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üåü Working with Complex Data Types in Spark SQL\n",
    "\n",
    "Spark SQL supports powerful complex data types like **arrays**, **maps**, and **structs**, which allow you to store and manipulate nested and multi-valued data directly inside tables and views.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ Arrays\n",
    "- Represent ordered collections of elements (e.g., a list of items in an order).\n",
    "- Useful for storing multiple values in a single column.\n",
    "- Can be **exploded** into multiple rows to process each element individually.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© Maps\n",
    "- Key-value pairs, similar to dictionaries in Python or objects in JSON.\n",
    "- Perfect for storing attributes with dynamic keys (e.g., product prices by item name).\n",
    "- Can be **exploded** to transform each key-value pair into a separate row.\n",
    "\n",
    "---\n",
    "\n",
    "### üü• Structs\n",
    "- Complex nested data structures, like JSON objects.\n",
    "- Group multiple related fields into a single column.\n",
    "- Access nested fields using dot notation (`struct_col.field_name`).\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Common Operations\n",
    "- **Exploding arrays and maps** lets you flatten complex data to analyze individual elements.\n",
    "- **Accessing nested struct fields** allows you to work with detailed, structured info inside a row.\n",
    "- These types enable flexible, scalable schemas ‚Äî essential for semi-structured or hierarchical data.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Why Use Complex Types in Spark SQL?\n",
    "\n",
    "- They reduce the need for complex joins by encapsulating related data.\n",
    "- Improve query expressiveness and performance by keeping data together.\n",
    "- Are widely used in modern data lake architectures, especially with formats like Parquet and Delta Lake.\n",
    "\n",
    "---\n",
    "\n",
    "Next, we'll demonstrate queries to **explode arrays and maps**, and **access struct fields** in practice. Suggestion: check also original view to see how each query looked before and after. \n",
    "Let's get hands-on! üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c60e91-415b-4e7a-867f-69a25a1c39ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exploding arrays ‚Äî return one row per item per order"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  order_id,\n",
    "  order_date,\n",
    "  item\n",
    "FROM orders_with_items\n",
    "LATERAL VIEW EXPLODE(items) AS item;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4cdecd6-345e-48a1-a824-879ee5ceed0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Accessing map values ‚Äî get price of ‚Äòapple‚Äô for each product"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  product_id,\n",
    "  price_map['apple'] AS apple_price\n",
    "FROM products_with_prices;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "782400d9-ee5a-4622-8f35-f25ccd491b74",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Getting keys and values from maps ‚Äî explode the map into key-value pairs"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  product_id,\n",
    "  key,\n",
    "  value\n",
    "FROM products_with_prices\n",
    "LATERAL VIEW EXPLODE(price_map) AS key, value;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bfafe4c-d9d4-4e13-8841-82909e0620eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Accessing nested struct fields"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  employee_id,\n",
    "  address.name AS employee_name,\n",
    "  address.street AS street,\n",
    "  address.state AS state\n",
    "FROM employees_with_address;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f09905e8-8e2a-4f18-8e6a-2914fbd038cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Important !!\n",
    "\n",
    "**Are we using any of this within our project ?**  \n",
    ".\n",
    ".\n",
    "**YES!** \n",
    "\n",
    "Whole Data Quality engine is build using map columns and beforementioned functions, in order to reduce size, complexity and storage/compute costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53601a29-435a-4fd9-bfee-52955bcd9f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Spark SQL intermedaite",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
