{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db91314d-23b7-4246-8694-7642947cc6ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 📁 Step 1: Creating Schemas to Organize Your Data\n",
    "\n",
    "In any professional data architecture — especially when using **Delta Live Tables (DLT)** — it’s critical to separate raw ingested data from modeled data for **clarity, governance, and scalability**.  \n",
    "This is where **schemas** come in. Think of them as folders that help you structure your data warehouse layers.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🗂️ Why We Create Separate Schemas:\n",
    "\n",
    "| 🔸 Schema        | 📌 Purpose |\n",
    "|------------------|------------|\n",
    "| `lake_schema`    | Stores **cleaned data** (aka. *staging/bronze layer*) |\n",
    "| `hub_schema`     | Stores **modeled and enriched data** (*star schema*, dimensions, facts) |\n",
    "\n",
    "✅ This separation makes it easier to:\n",
    "- Apply different data quality expectations 🧪  \n",
    "- Track lineage 🔄  \n",
    "- Manage permissions 🔐  \n",
    "- Keep your modeling layer clean and focused 🎯  \n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ SQL Commands to Create the Schemas:\n",
    "\n",
    "after running following script check you unity catalog and you should see 2 new empty schemas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a92ba4-c48c-44b2-93e7-d6854be1c0db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create schemas"
    }
   },
   "outputs": [],
   "source": [
    "create SCHEMA if not exists hub_schema;\n",
    "create schema if not exists lake_schema;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ab936c-e78f-416b-8513-9545d13aebf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ✅ Step 2: Running the DLT Pipeline – Building the Star Schema\n",
    "\n",
    "Now that we have defined our data model using Delta Live Tables (DLT), let’s walk through what we’ve achieved in the previous steps and clarify the design:\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 DLT Table Structure\n",
    "\n",
    "We defined the following DLT tables:\n",
    "\n",
    "#### 🔹 Lake Layer (Staging):\n",
    "- `lake_schema.customers`\n",
    "- `lake_schema.products`\n",
    "- `lake_schema.sales`\n",
    "\n",
    "#### 🌟 Hub Layer (Modeled):\n",
    "- `hub_schema.dim_customers` — Customer dimension with a **Primary Key**.\n",
    "- `hub_schema.dim_products` — Product dimension with a **Primary Key**.\n",
    "- `hub_schema.fact_sales_star` — Fact table with denormalized dimension data and a **row_key as Primary Key**.\n",
    "\n",
    "> 💡 In Unity Catalog, we defined **Primary Keys** directly in the table declarations using `PRIMARY KEY (...)`. After running pipeline go and see unity catalog for newly created tables.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Foreign Key Limitation\n",
    "\n",
    "Due to current [Unity Catalog](https://docs.databricks.com/en/data-governance/unity-catalog/index.html) limitations:\n",
    "- **Foreign keys are not yet fully enforced**, even though we can define them in table metadata.\n",
    "- That's why we will **manually enforce referential integrity** in the next step using SQL checks and joins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0775123e-ee95-461d-baf1-14d1bd0e06c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ▶️ Let’s now run the DLT Pipeline in notebook `6.1` to materialize our model tables.\n",
    "\n",
    "\n",
    "1. go to Jobs & Pipelines -> create new pipeline \n",
    "2. in first window select whole directory of this training\n",
    "3. in second window select specifically notebook 6.1 \n",
    "4. it will create new UI and you can DLT pipeline directly form notebook and observe its graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70274975-8cd5-4b8b-8b3e-0275f445236a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🔗 Adding **Foreign Keys** to Strengthen Our Data Model \n",
    "\n",
    "So far, we’ve created the **`hub_schema`** with our **model tables**, and defined **primary keys** to uniquely identify each record.  \n",
    "However, we haven’t yet added **foreign key relationships** — these are essential to describe how our **fact table** connects to the **dimension tables**.  \n",
    "\n",
    "> ⚠️ This limitation is due to Delta Live Tables (DLT) table definitions.  \n",
    "> In our project, we use an API call to automatically update foreign keys *after* a new tables are created.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why add **Foreign Keys**? 🤔\n",
    "\n",
    "- 📝 **Document** relationships clearly between tables  \n",
    "- 🛡️ **Improve data integrity** by showing how dimensions relate to facts  \n",
    "- 🔍 Make your data model **easier to understand and maintain**\n",
    "\n",
    "> 💡 **Note:**  \n",
    "> Many platforms (including DLT) may *not* fully enforce foreign key constraints, but **adding them is best practice** for clarity and future-proofing your architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "Change and Run the script (6.1 last script for fact table) to add foreign keys to your fact table\n",
    "\n",
    "**FROM**\n",
    "```\n",
    "customer_id INTEGER,\n",
    "product_id INTEGER,\n",
    "--customer_id INTEGER FOREIGN KEY REFERENCES hub_schema.dim_customers(customer_id),\n",
    "--product_id INTEGER FOREIGN KEY REFERENCES hub_schema.dim_products(product_id),\n",
    "```\n",
    "**TO**\n",
    "```\n",
    "--customer_id INTEGER,\n",
    "--product_id INTEGER,\n",
    "customer_id INTEGER FOREIGN KEY REFERENCES hub_schema.dim_customers(customer_id),\n",
    "product_id INTEGER FOREIGN KEY REFERENCES hub_schema.dim_products(product_id),\n",
    "```\n",
    "- This way after tables which are referenced by FK constraint are already created in first pipeline run and constraint is enforced in second run. \n",
    "\n",
    "then:  \n",
    "\n",
    "➡️ **Go check the ERD (Entity Relationship Diagram) in Unity Catalog!**  \n",
    "You’ll now see the **relationships between your fact and dimension tables visualized**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🎉 Congratulations! 🎉\n",
    "\n",
    "You’ve just built a **simple Star Schema** in DLT — a solid foundation for scalable, well-structured data modeling! 👏👏\n",
    "\n",
    "---\n",
    "\n",
    "If you have any questions, don’t hesitate to ask — mastering relationships is key to becoming a great data modeler! 💪😊\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e28033ae-8e0e-4c4b-9323-3fa3ff942340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🙏 Thank You for Your Attention! 💫\n",
    "\n",
    "We know this training covered a lot of ground — and that’s totally normal!  \n",
    "If you don’t understand everything right away, **don’t be discouraged**. Learning complex data modeling and pipelines takes time and practice.\n",
    "\n",
    "Your feedback is super valuable to us!  \n",
    "If you feel something is missing, or if you spot any bugs during the training, please let us know so we can improve it for future versions. 🙌\n",
    "\n",
    "---\n",
    "\n",
    "### 🧹 Cleanup Time\n",
    "\n",
    "Before you go, please run the following script to **delete the schemas** we created during this modeling training.  \n",
    "This helps keep your environment clean and ready for the next session!\n",
    "\n",
    "**Dont forget to delete Pipeline also from UI!!** same as you have created pipeline in steps before go to Jobs & Pipelines. You should see just your created pipeline. click on 3 vertical dots on far right and DELETE.   \n",
    "\n",
    "---\n",
    "\n",
    "If you have any questions or want to revisit any part of this training, just reach out — we’re here to support you! 🌟\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc76302-99e1-4dac-be1d-621ee844598f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "clean up"
    }
   },
   "outputs": [],
   "source": [
    "drop schema if exists hub_schema CASCADE;\n",
    "drop schema if exists lake_schema CASCADE;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6.0 Modeling practice prerequisites",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
