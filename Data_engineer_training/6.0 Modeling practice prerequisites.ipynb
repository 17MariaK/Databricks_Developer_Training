{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db91314d-23b7-4246-8694-7642947cc6ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 📁 Step 1: Creating Schemas to Organize Your Data\n",
    "\n",
    "In any professional data architecture — especially when using **Delta Live Tables (DLT)** — it’s critical to separate raw ingested data from modeled data for **clarity, governance, and scalability**.  \n",
    "This is where **schemas** come in. Think of them as folders that help you structure your data warehouse layers.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🗂️ Why We Create Separate Schemas:\n",
    "\n",
    "| 🔸 Schema        | 📌 Purpose |\n",
    "|------------------|------------|\n",
    "| `raw_schema`    | Stores **raw data** (aka. *staging/potato layer*) |\n",
    "| `lake_schema`    | Stores **cleaned data** (aka. *cleaned/bronze layer with metadata columns*) |\n",
    "| `hub_schema`     | Stores **modeled and enriched data** (aka. *silverstar schema*, dimensions, facts) |\n",
    "\n",
    "✅ This separation makes it easier to:\n",
    "- Apply different data quality expectations 🧪  \n",
    "- Track lineage 🔄  \n",
    "- Manage permissions 🔐  \n",
    "- Keep your modeling layer clean and focused 🎯  \n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ SQL Commands to Create the Schemas:\n",
    "\n",
    "after running following script check you unity catalog and you should see 3 new empty schemas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a92ba4-c48c-44b2-93e7-d6854be1c0db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create schemas"
    }
   },
   "outputs": [],
   "source": [
    "create SCHEMA if not exists hub_schema;\n",
    "create schema if not exists lake_schema;\n",
    "create schema if not exists raw_schema;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc1a65d1-9c17-401c-a168-67215c5b95f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  🛠️ Step 2: Ingest data from Github\n",
    "This exercise works with ingested data from Github ( fixtures )\n",
    "Following code will ingest data in its raw form to raw layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b271d648-8dc4-4fa9-a5ba-d545c860530d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ingest data from github"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "\n",
    "table_list = [\"customer\", \"product\", \"sales\"]\n",
    "\n",
    "for table in table_list:\n",
    "    url = f\"https://raw.githubusercontent.com/VladisKliman/Databricks_Developer_Training/main/fixtures/{table}.csv\"\n",
    "    pdf = pd.read_csv(url)\n",
    "    df = spark.createDataFrame(pdf)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"raw_schema.raw_{table}\")\n",
    "\n",
    "    spark.sql(f\"ALTER TABLE raw_schema.raw_{table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ab936c-e78f-416b-8513-9545d13aebf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ▶️ Step 3: Building the Star Schema\n",
    "\n",
    "Now that we have defined our data model using Delta Live Tables (DLT), let’s walk through what we will build in the next steps and clarify the design:\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 DLT Table Structure\n",
    "\n",
    "We defined the following DLT tables:\n",
    "\n",
    "#### 🔹 Lake Layer (Staging):\n",
    "- `lake_schema.customers`\n",
    "- `lake_schema.products`\n",
    "- `lake_schema.sales`\n",
    "\n",
    "#### 🌟 Hub Layer (Modeled):\n",
    "- `hub_schema.dim_customers` — Customer dimension with a **Primary Key**.\n",
    "- `hub_schema.dim_products` — Product dimension with a **Primary Key**.\n",
    "- `hub_schema.fact_sales_star` — Fact table with denormalized dimension data and a **row_key as Primary Key**.\n",
    "\n",
    "> 💡 In Unity Catalog, we defined **Primary Keys** directly in the table declarations using `PRIMARY KEY (...)`. After running pipeline go and see unity catalog for newly created tables.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Foreign Key Limitation\n",
    "\n",
    "Due to current [Unity Catalog](https://docs.databricks.com/en/data-governance/unity-catalog/index.html) limitations:\n",
    "- **Foreign keys are not yet fully enforced**, even though we can define them in table metadata.\n",
    "- That's why we will **manually enforce referential integrity** in the additional step using SQL script adjustement after tables are build.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0775123e-ee95-461d-baf1-14d1bd0e06c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####  Running the DLT Pipeline\n",
    "\n",
    "Follow these steps to run the pre-built Delta Live Tables (DLT) pipeline using an existing notebook:\n",
    "\n",
    "1. Go to **Jobs & Pipelines** → click **Create New Pipeline**.\n",
    "2. If you don’t see the **LakeFlow Pipeline Editor**, enable it using the toggle at the **top center** of the screen.\n",
    "3. Name your pipeline and click **Add existing assets**.\n",
    "4. In the first window (**Pipeline root folder**), select the **entire directory** that contains this training material.\n",
    "5. In the second window (**Source code paths**), select **only notebook 6.1**.\n",
    "6. A new UI will be created where you can run the **DLT pipeline directly from the notebook** and observe its execution graph.\n",
    "\n",
    "> ✅ This allows you to visually inspect how the pipeline runs, including dependencies between tables and processing logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70274975-8cd5-4b8b-8b3e-0275f445236a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🔗 Step 4: Adding **Foreign Keys** to Strengthen Our Data Model \n",
    "\n",
    "So far, we’ve created the **`hub_schema`** with our **model tables**, and defined **primary keys** to uniquely identify each record.  \n",
    "However, we haven’t yet added **foreign key relationships** — these are essential to describe how our **fact table** connects to the **dimension tables**.  \n",
    "\n",
    "> ⚠️ This limitation is due to Delta Live Tables (DLT) table definitions.  \n",
    "> In our project, we use an API call to automatically update foreign keys *after* a new tables are created.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why add **Foreign Keys**? 🤔\n",
    "\n",
    "- 📝 **Document** relationships clearly between tables  \n",
    "- 🛡️ **Improve data integrity** by showing how dimensions relate to facts  \n",
    "- 🔍 Make your data model **easier to understand and maintain**\n",
    "\n",
    "> 💡 **Note:**  \n",
    "> Many platforms (including DLT) may *not* fully enforce foreign key constraints, but **adding them is best practice** for clarity and future-proofing your architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "Change and Run the script (6.1 last script for fact table) to add foreign keys to your fact table\n",
    "\n",
    "**FROM**\n",
    "```\n",
    "customer_fk STRING,\n",
    "  product_fk STRING,\n",
    "  --customer_fk STRING FOREIGN KEY REFERENCES hub_schema.dim_customer(row_key) COMMENT 'FK to customer dimension',\n",
    "  --product_fk STRING FOREIGN KEY REFERENCES hub_schema.dim_product(row_key) COMMENT 'FK to product dimension',\n",
    "```\n",
    "**TO**\n",
    "```\n",
    "--customer_fk STRING,\n",
    "--product_fk STRING,\n",
    "customer_fk STRING FOREIGN KEY REFERENCES hub_schema.dim_customer(row_key) COMMENT 'FK to customer dimension',\n",
    "product_fk STRING FOREIGN KEY REFERENCES hub_schema.dim_product(row_key) COMMENT 'FK to product dimension',\n",
    "```\n",
    "- This way after tables which are referenced by FK constraint are already created in first pipeline run and constraint is enforced in second run. \n",
    "\n",
    "then:  \n",
    "\n",
    "➡️ **Go check the ERD (Entity Relationship Diagram) in Unity Catalog!**  \n",
    "You’ll now see the **relationships between your fact and dimension tables visualized**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🎉 Congratulations! 🎉\n",
    "\n",
    "You’ve just built a **simple Star Schema** in DLT — a solid foundation for scalable, well-structured data modeling! 👏👏\n",
    "\n",
    "---\n",
    "\n",
    "If you have any questions, don’t hesitate to ask — mastering relationships is key to becoming a great data modeler! 💪😊\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e28033ae-8e0e-4c4b-9323-3fa3ff942340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: 🙏 Thank You for Your Attention! 💫\n",
    "\n",
    "We know this training covered a lot of ground — and that’s totally normal!  \n",
    "If you don’t understand everything right away, **don’t be discouraged**. Learning complex data modeling and pipelines takes time and practice.\n",
    "\n",
    "Your feedback is super valuable to us!  \n",
    "If you feel something is missing, or if you spot any bugs during the training, please let us know so we can improve it for future versions. 🙌\n",
    "\n",
    "---\n",
    "\n",
    "### 🧹 Cleanup Time\n",
    "\n",
    "Before you go, please run the following script to **delete the schemas** we created during this modeling training.  \n",
    "This helps keep your environment clean and ready for the next session!\n",
    "\n",
    "**Dont forget to delete Pipeline also from UI!!** same as you have created pipeline in steps before go to Jobs & Pipelines. You should see just your created pipeline. click on 3 vertical dots on far right and DELETE.   \n",
    "\n",
    "---\n",
    "\n",
    "If you have any questions or want to revisit any part of this training, just reach out — we’re here to support you! 🌟\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc76302-99e1-4dac-be1d-621ee844598f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "clean up"
    }
   },
   "outputs": [],
   "source": [
    "drop schema if exists hub_schema CASCADE;\n",
    "drop schema if exists lake_schema CASCADE;\n",
    "drop schema if exists raw_schema CASCADE;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6.0 Modeling practice prerequisites",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
