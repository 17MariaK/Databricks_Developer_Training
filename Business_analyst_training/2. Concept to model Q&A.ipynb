{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b34cfd0-832d-419c-a235-79c2ed836cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß≠ From Business Need to Data Model ‚Äì Decision Flow for Analysts & Engineers\n",
    "\n",
    "When implementing a new data model, **Business Analysts** and **Data Engineers** need to collaborate closely. To ensure the model supports the business case, it's important to answer a structured set of questions together before stories and tasks could be confirmed and specified in Jira.\n",
    "In this section **we will go through set of questions** which should be answered. For clarity from **technical perspective please refer to previous sections** where in basic way each technicality is described.\n",
    "\n",
    "For more clarity on responsibilities and overall flow : https://onetakeda.atlassian.net/wiki/spaces/GMSGQDIME/pages/6366625819/Business+Analysis+Step-by-Step\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ 1. What are the Data Sources?\n",
    "\n",
    "| Question | Why It Matters |\n",
    "|----------|----------------|\n",
    "| Are we sourcing from **SAP**, **CSV**, **API**, **IoT**, or **External DBs**? | Affects ingestion method, latency, and transformations. |\n",
    "| Is it **Structured** or **Unstructured** data, does it contain LOBs ?  | Determines complexity of solution. |\n",
    "| Do we have **GxP/SOX** requirements, specific **data contracts** to follow? | Determines compliance architecture. |\n",
    "| Do we have **already existing connection**? | Determines effort to add new resource. |\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ 2. What Type of Data Refresh is Needed?\n",
    "\n",
    "| Option | Description |\n",
    "|--------|-------------|\n",
    "| **Batch Processing** | Data updates periodically (e.g., daily or hourly). |\n",
    "| **Streaming / Near Real-Time** | Required for use cases like monitoring, dashboards, alerts. Implemented with DLT + MSK (Kafka) or LakeFlow Connect. |\n",
    "| **Is Underlying database/source prepared with capacity for providing data?** | Especially in streaming. Event logs should be available to incrementaly load data and Database should handle extra load. |\n",
    "---\n",
    "\n",
    "### üß© 2. What Kind of Data Model is Required?\n",
    "\n",
    "| Question | Why It Matters |\n",
    "|---------|----------------|\n",
    "| Is this a **Star** or **Snowflake** schema? | Affects table structure and performance optimization. |\n",
    "| Will the model include **dimensions** and **facts**? | Determines normalization and fact-to-dim joins. |\n",
    "| Does it require **data from other domains or teams**? | Dependency tracking, data availability, responsibility across teams. |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è 3. What Is the Load Strategy?\n",
    "\n",
    "| Question | Why It Matters |\n",
    "|----------|----------------|\n",
    "| Do we need to track changes using **SCD Type 1**, **1.5**, or **2**? | Impacts schema design ‚Äî SCD2 requires historical tracking with versioning or date ranges. |\n",
    "| Does the source system support **CDC (Change Data Capture)**? | Required to detect changes for incremental loads, especially with SCD2. |\n",
    "| Are **deletes** tracked or soft-deleted in source? | Important for data retention logic and historical correctness. |\n",
    "\n",
    "---\n",
    "\n",
    "### üßº 4. Does the Data Require Standardization, Quality Checks, or Access Control?\n",
    "\n",
    "Standardization and compliance are critical, especially in regulated environments like pharma. This step ensures your data is **trusted**, **consistent**, and **secure**.\n",
    "\n",
    "| Question | Why It Matters |\n",
    "|----------|----------------|\n",
    "| Does the data need **standardization via MDM mapping/lookup of golden values**? | Is mapping needed ? Is there existing mapping or new should be implemented ( creates dependency on MDM). |\n",
    "| Are **business data quality checks** required (complex business rules besides simple technical checks)? | Prevents incorrect KPIs and flawed insights. Implemented via DQH Engine or pipeline expectations. |\n",
    "| Does the data require **masking, obfuscation, or row-level filtering**? | Critical for PII, sensitive data, and regulatory compliance (e.g., GxP, GDPR). |\n",
    "| Do we need to **tag or label tables/columns** for governance? | Enables lineage tracking, compliance audits, and discoverability in Unity Catalog. Implementation with One Data Catalog |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### üìä 5. Will the Data Be Used for Dashboarding or API Consumption?\n",
    "\n",
    "| Question | Why It Matters |\n",
    "|----------|----------------|\n",
    "| Is the data consumed by **dashboards (e.g., Power BI, Tableau)**? | Requires fast aggregations, clean joins, and user-friendly fields. |\n",
    "| Will it serve **API endpoints or data products**? | Requires stable schemas, consistent latency, and simplified result sets within mart layer. |\n",
    "| Is the data **aggregated or raw**? | Affects performance, cost, and how modeling logic is applied. |\n",
    "\n",
    "---\n",
    "### üó∫Ô∏è 6. Documentation ( STTM source to target mapping)\n",
    "| Concept | Why It Matters |\n",
    "|----------|----------------|\n",
    "|Functional understanding of column values |In order to efectivelly work with data, DE should be informed about what purpose each column serves. |\n",
    "| naming convention, documentation | before data engineer can build pipeline, business logic ( transformation, sourcing) needs to be documented ba BAs. For more information please check this link: https://onetakeda.atlassian.net/wiki/spaces/GMSGQDIME/pages/6125487467/Databricks+Naming+Standardization+App+-+User+Guide|\n",
    "| original table analysis ( types, naming) | current process and helper application described in here : https://onetakeda.atlassian.net/wiki/spaces/GMSGQDIME/pages/5980553844/Databricks+Data+Profiler+App+-+User+Guide|\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "####Data Domain Products specification documentation :\n",
    "- https://mytakeda.sharepoint.com/:x:/s/GMSGQDataAnalyticsDigital/EeU0gx9VXaNLm1XollWf97YBUb4OUFjO8TXeIDdWbQCwnA?e=hrmwhx&CID=A4338547-F53A-4070-9142-E61871CF3AA0&wdLOR=c4ED83762-7CAC-4108-B109-7D016AD1BF86\n",
    "\n",
    "---\n",
    "\n",
    "üìå Once these questions are aligned, the engineering team can confidently proceed with implementation using the right modeling, tools (DLT, Unity Catalog, etc.), and schedule.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Concept to model Q&A",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
