{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce354708-a2ae-42f0-b30f-f3060e387ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔹 What is Apache Spark?\n",
    "\n",
    "Apache Spark is an open-source distributed computing system used for big data processing and analytics. It’s known for its speed, ease of use, and ability to process large-scale data.\n",
    "\n",
    "### Key Concepts:\n",
    "**Distributed computing:** Spark processes data in parallel across many machines.\n",
    "\n",
    "**In-memory processing:** Unlike Hadoop, Spark can keep data in memory for faster access.\n",
    "\n",
    "**Unified engine:** Supports multiple tasks like batch processing, streaming, machine learning, and SQL.\n",
    "\n",
    "### Spark Ecosystem Components:\n",
    "\n",
    "| **Component**     | **Description** |\n",
    "|-------------------|-----------------|\n",
    "| **Spark SQL**     | Run SQL queries on large datasets. Supports structured and semi-structured data. |\n",
    "| **Spark Core**    | The foundation – handles basic I/O, scheduling, and task execution. |\n",
    "| **Spark Streaming** | For processing real-time data streams. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc9b963-8c60-4dff-9a9f-65194e03755f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔹 Parallelism\n",
    "### 🍬 Real-Life Analogy: Counting M&M Lentils with Spark\n",
    "\n",
    "Imagine you're organizing a big event and you receive a **huge bag of mixed M&M lentils**—millions of them.\n",
    "\n",
    "Your goal? 👉 Count how many M&Ms there are of each color.\n",
    "\n",
    "You don't want to do this alone—it would take forever. So, you call your friends (Spark Cluster) to help you out!\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 The Spark Cluster Analogy\n",
    "\n",
    "| **Spark Component** | **Real-Life Equivalent** |\n",
    "|---------------------|--------------------------|\n",
    "| **Driver Program**  | You (the event organizer giving instructions) |\n",
    "| **Executors**       | Your friends helping count M&Ms |\n",
    "| **Cluster Manager** | Your group chat deciding who does what |\n",
    "| **Tasks**           | \"Count the red ones from this bucket\" |\n",
    "| **Partitions**      | Separate bowls of M&Ms to divide the work |\n",
    "\n",
    "---\n",
    "\n",
    "#### 🛠️ How the Work is Distributed\n",
    "\n",
    "1. **Driver Program (You)**:  \n",
    "   You write down a plan: \"Count how many red, blue, green, yellow M&Ms there are.\"\n",
    "\n",
    "2. **Divide the Work (Partition the Data)**:  \n",
    "   You pour the big bag into 4 bowls (data partitions), one for each friend.\n",
    "\n",
    "3. **Send Tasks to Executors**:  \n",
    "   You give each friend a simple task:  \n",
    "   \"Count how many of each color you find in your bowl.\"\n",
    "\n",
    "4. **Friends Start Counting (Parallel Processing)**:  \n",
    "   All friends count at the same time – no waiting!  \n",
    "   Each friend returns a small result like:  \n",
    "   - Friend 1: Red – 105, Blue – 87, Green – 94  \n",
    "   - Friend 2: Red – 98, Blue – 102, Green – 101  \n",
    "   *(...and so on)*\n",
    "\n",
    "5. **Combine the Results (Aggregation at Driver)**:  \n",
    "   You collect all the partial counts and add them together:  \n",
    "   - Total Red = 105 + 98 + ...  \n",
    "   - Total Blue = 87 + 102 + ...\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Spark Benefits Shown Here:\n",
    "\n",
    "- **Speed**: Counting is done in parallel.\n",
    "- **Scalability**: You can invite more friends (add executors) if the bag gets bigger.\n",
    "- **Efficiency**: You only do the final summation (aggregation).\n",
    "\n",
    "---\n",
    "\n",
    "#### 📌 In Real Spark SQL\n",
    "\n",
    "You might run:\n",
    "```sql\n",
    "SELECT color, COUNT(*) FROM m_and_ms GROUP BY color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27b774f-3c12-4ddb-8c6e-38575d7281ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔹 Partitioning\n",
    "---\n",
    "So how can we help with efficiency and scalability within code ? first answer is propper table partitioning.  \n",
    "### 🧩 Table Partitioning: Distributing Work Evenly\n",
    "\n",
    "Continuing our M&M analogy 🍬...\n",
    "\n",
    "Let’s say you again have 4 friends helping to count M&Ms. But this time, you accidentally pour 90% of the M&Ms into **one** bowl, and only small amounts into the other 3.\n",
    "\n",
    "Now what happens?\n",
    "\n",
    "- One friend is **overwhelmed** with M&Ms. 😩\n",
    "- The other three finish quickly and **sit idle**. 🪑\n",
    "- The final result takes much longer than necessary. 🕒\n",
    "\n",
    "This is called **data skew** — when some partitions have much more data than others.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 What is Table Partitioning?\n",
    "\n",
    "In Spark (and Delta Lake / Parquet), **partitioning** means physically organizing data files **by column values**, typically in the file system (e.g., `/table/date=2025-07-18/`).\n",
    "\n",
    "Partitioning allows Spark to:\n",
    "\n",
    "- **Prune unnecessary data** (read only what's needed)\n",
    "- **Parallelize processing efficiently**\n",
    "- **Avoid scanning full tables**\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Good Partitioning Example\n",
    "\n",
    "If you're querying a table by **`event_date`** most of the time, you could partition by `event_date`.\n",
    "\n",
    "That way, when running:\n",
    " \n",
    "\n",
    "```%sql\n",
    "SELECT * FROM events WHERE event_date = '2025-07-18'```\n",
    "\n",
    ".. you can skip chunks of data which you are not interested in and ease memory utilization ( instead of reading it all and filtering , you can just pick what you need)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eeaf802-168a-4b01-8b24-205de834b6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ⚠️ What Happens with Bad Partitioning?\n",
    "\n",
    "Let’s explore **two common partitioning mistakes**:\n",
    "\n",
    "---\n",
    "\n",
    "#### ❌ 1. Partitioning by a Skewed Column\n",
    "\n",
    "Let’s say you partition by `user_id`, and one user (e.g., `user_42`) has 90% of the activity.\n",
    "\n",
    "- Spark writes most of the data into one partition (`user_id=42`).\n",
    "- That partition takes much longer to process.\n",
    "- Other partitions are idle, causing **data skew** and **slow jobs**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ❌ 2. Partitioning by a Unique Key (e.g., Primary Key)\n",
    "\n",
    "Let’s say you partition by `transaction_id` or `order_id` — which are **unique for each row**.\n",
    "\n",
    "- Spark creates **one folder per row** (partition).\n",
    "- You end up with **millions of small files** and folders.\n",
    "- Metadata operations become slow (e.g., listing files, updating table state).\n",
    "- Queries are slower because Spark must scan lots of tiny files.\n",
    "- Writes can fail or severely degrade due to **filesystem overhead**.\n",
    "\n",
    "📉 This is called the **small file problem** — and it kills performance at scale.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Key Insight\n",
    "\n",
    "> **Just because a column is important (like a primary key) doesn’t mean it should be used for partitioning.**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Better Alternatives\n",
    "\n",
    "| Instead of this...                | Try this...                        |\n",
    "|----------------------------------|------------------------------------|\n",
    "| Partitioning by `transaction_id` | Don’t partition, or use `event_date` |\n",
    "| Partitioning by `user_id`        | Use `country`, `region`, or `user_type` if more balanced |\n",
    "| No partitioning at all           | Partition by columns in common filters (e.g., `event_date`, `category`) |\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Summary\n",
    "\n",
    "> Partitioning helps **divide the work**, but poor partitioning creates more problems than it solves.\n",
    "\n",
    "Always aim for:\n",
    "- Balanced partition sizes\n",
    "- Not too many partitions\n",
    "- Predictable, filterable values\n",
    "\n",
    "Spark loves **\"just right\" partitioning** — not too many, not too few.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b7b871-7e5e-40fa-a5c5-fe1ed783d3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔹 Excercise 1.\n",
    "- create dataframe/temporary view with your username so its unique \n",
    "- create 2 tables\n",
    "  - partitioned by primary key\n",
    "  - correctly partitioned by event_date\n",
    "- check table definitions\n",
    "- observe load times when reading from tables  \n",
    "- clear resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee6f3c4-833f-491e-a73e-7421e9d69a1e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create training dataframe"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import expr\n",
    "from datetime import datetime\n",
    "\n",
    "# build name for your table\n",
    "table_name = spark.sql(\"select replace(split(current_user(), '@')[0],'.','') AS short_name\").collect()[0]['short_name']\n",
    "table_base = f\"gmsgq_training_{table_name}_sales\"\n",
    "\n",
    "# Create demo DataFrame\n",
    "data = spark.range(0, 10000).selectExpr(\n",
    "    \"id as order_id\",\n",
    "    \"CAST(id % 50 AS STRING) AS user_id\",  # 50 users\n",
    "    \"date_sub(current_date(), CAST(id % 10 AS INT)) AS event_date\",\n",
    "    \"CAST(rand() * 100 AS DECIMAL(10,2)) AS amount\"\n",
    ")\n",
    "\n",
    "data.createOrReplaceTempView(\"tmp_sales\")\n",
    "\n",
    "print(f\"Base table name: {table_base}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80946c82-4a10-44a7-b86c-942252fd7f9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create well partitioned table"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE ${table_base}_good_partitioning\n",
    "USING DELTA\n",
    "PARTITIONED BY (event_date) -- partitioned by event_date (well distributed across workers, alows to be pruned when filtered by where statement)\n",
    "AS\n",
    "SELECT * FROM tmp_sales;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41a1fe5-8103-4e71-a9dd-f4b362184495",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":246},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752834291889}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE table ${table_base}_good_partitioning;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c04b2f-69e7-4a3d-b001-fd8606a6129b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create badly partitioned table"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE ${table_base}_bad_partitioning \n",
    "USING DELTA\n",
    "PARTITIONED BY (order_id) -- partitioned by primary Key ( big problem)\n",
    "AS \n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  tmp_sales;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a36b63e-dd5b-4a3d-8d44-3a8a5ee0939f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE TABLE ${table_base}_bad_partitioning;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88a50e5-ef1a-4796-9986-a7c5bbfea995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Filter by event_date (partition pruning works!)\n",
    "SELECT COUNT(*) FROM ${table_base}_good_partitioning WHERE event_date = current_date();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a6185a-a1e7-41e1-ae97-0213926e05d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Try filtering on order_id (no partition pruning)\n",
    "SELECT COUNT(*) FROM ${table_base}_bad_partitioning WHERE order_id < 123;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2290e7d1-cd26-40c7-b945-db59ca29df65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drop table if exists ${table_base}_good_partitioning;\n",
    "drop table if exists ${table_base}_bad_partitioning;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "first steps with spark",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
